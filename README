Useful papers and data: https://drive.google.com/drive/u/0/folders/1qnYOLpFim9gUH7K8m2bvkeENi474ZKyS

TODO:
Error severity index
Power calculation (sample size estimation)
Label the axes of the ONEST plot
Legend for datasets
Identify the crossover point between 2 curves.


Multiclass Ordinal Metrics Document
https://docs.google.com/document/d/1_v-9WWoWL7Q4XFFECHozLi_WRLNQETD1Z3ZbdIEET38/edit

## From: David Jin, 18 October 2022
## Jansen and Niels,
#
## Turns out that the question you are interested in for your metrics is called 
## ordinal regression. You can find a primer about it on Wikipedia
## https://en.wikipedia.org/wiki/Ordinal_regression.
#
## Unfortunately most of the metrics for ordinal regression are loss functions
## aimed at training AI models. Thus, they are not nicely bounded between 0
## and 1. I worked out some of the math on an alternative reasonable metric.
## I’ve attached a short explanation which I’m happy to walk through on a call
## if you would like.
#
## Basically, if you want a function that takes in a dataset of pairs of ground
## truth and predicted labels, you always get something of a fixed form, which
## is intuitively like a “generalized accuracy”, “generalized precision” or
## “generalized recall”. For an example, I’ll describe “generalized recall”. In
## normal recall (true positives over all positives), all true positives
## contribute 1 point and all false negatives contribute 0 points. In
## generalized recall, each type of mistake is penalized different based on the
## distance of the predicted label from the actual label. A minor
## misclassification may contribute 0.8 points, while the worst misclassification
## may contribute 0 points.
#
## Anyways the above is a reasonably obvious metric to come up with. The
## interesting thing is that if you want a function which comes out between 0
## and 1 and satisfies some intuitive properties (like all elements of the
## dataset are weighted equally, if you get everything right you should get 1,
## etc.), it’s the only reasonable function functional form.
#
## Sincerely,
## David

# To do:
# * how to manage 
# * The cost of misclassification is dependent on the within-case prevelance
# * Low prevalence adverse findings are the driving issue.
# * Look into connected components, other ways to handle dependencies
# * * patches on a slide
# * * slides in a block
# * * blocks from a part
# * * parts from a case
# * * cases from a patient
# * Multi-rater
#   “My views on this are more in line with Grove et al. (1981) who while
#    talking about what diagnosticians in the medical field actually do said
#    this: “They assign the easy cases or “textbook” cases, to diagnoses with
#    little or no error; they may guess or diagnose randomly on the others. If
#    one knew which cases were textbook cases, one could treat them separately;
#    but that is a difficult matter.” 
#                                 -- Gwet’s Handbook of Inter-Rater ReLiability

# Preface
None of the errors herein are mine, the editors are conspiring against me.

# Expected Folders and Setup
- data/prostate_reader/assisted_5class.npy
- data/prostate_reader/unassisted_5class.npy

# Original Data
- data/
    - prostate_reader/
        - assisted_5class.csv
        - unassisted_5class.csv
    - nottingham/
        - mitosis.csv
        - nottingham.csv
        - pleomorphismus.csv
        - tubulus.csv

# Instructions
The analysis starts with `onest_analysis.py` to perform the ONEST and/or CONTEST analyses on the CSV data.
Make sure to cache this to create the appropriate NPYs used in the other programs.

# TODO
- Walk through the data folder for the data
- How did I convert `.csv` to `.npy`?
- Do better `get_args`
- Automatically create sarapes in alpha.py:get_data
- Choose consistent way to convert list to ndarray (note when other is necessary)
    - `np.array`
    - `np.asarray`
    - `np.empty` -> fill
- Choose consistent way to execute function over ndarray (`alpha.py:289`, `alpha.py:144`, `alpha.py:144`)
- Choose consistent way to identify assissted/unassisted or treatment/control
- Choose consistent style of docstring
- Figure out a consistent style of execution workflow (or decide to give up on it)
- Add detailed docstrings with parameter and return types

- Cleanup
- `pip3 freeze > requirements.txt`
- includes `examples` folder
    - Add
        - prostate_reader (DOI: 10.1001/jamanetworkopen.2020.23267)
        - nottingham (DOI: 10.1016/j.prp.2021.153718)
        - [PDL1](https://cran.r-project.org/web/packages/ONEST/index.html) (doi:10.1038/s41379-020-0544-x)
- Write up instructions on running some data from the beginning
- Get dad to run PDL1 from instructions I write up

```python
for root, subdirs, files in os.walk(sys.argv[1]):
    for filename in files:
        path = os.path.join(root, filename)
        nameBool, ext = nameCheck(path)
```

# Future Considerations
We calculate the OPA as the proportion of the number of observer agreements to total number of cases. There may be multiple ways to calculate this. The [FDA discuss overall percent agreement][fda-opa] in a 2-class positive vs. negative context.

[fda-opa]: https://www.fda.gov/files/medical%20devices/published/Guidance-for-Industry-and-FDA-Staff---Statistical-Guidance-on-Reporting-Results-from-Studies-Evaluating-Diagnostic-Tests-%28PDF-Version%29.pdf
[slides]: https://docs.google.com/presentation/d/1b7S_4nVgq0DsrQRkU7vEaCQtnOKafUGa/edit#slide=id.g22e7b4ae203_0_166